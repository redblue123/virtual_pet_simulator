#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„ _prioritized_sample æ–¹æ³•
"""

from pet import IntelligentPet
import numpy as np

print("=== æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿä¿®å¤ ===")

# åˆ›å»ºæ™ºèƒ½å® ç‰©
pet = IntelligentPet('æµ‹è¯•å® ç‰©')
print("âœ“ å® ç‰©åˆ›å»ºæˆåŠŸ")

# è·å–å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ
rl = pet.reinforcement_learning
print("âœ“ å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

# æµ‹è¯•åœºæ™¯1ï¼šç©ºç¼“å†²åŒº
print("\n=== æµ‹è¯•åœºæ™¯1ï¼šç©ºç¼“å†²åŒº ===")
sample = rl._prioritized_sample(32)
print(f"âœ“ ç©ºç¼“å†²åŒºé‡‡æ ·ç»“æœ: {sample}")

# æµ‹è¯•åœºæ™¯2ï¼šæ·»åŠ ä¸€äº›ç»éªŒ
print("\n=== æµ‹è¯•åœºæ™¯2ï¼šæ·»åŠ ç»éªŒå¹¶æµ‹è¯•é‡‡æ · ===")

# æ·»åŠ ä¸€äº›ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº
for i in range(10):
    # æ¨¡æ‹Ÿç»éªŒ
    state = rl.get_discrete_state()
    action = 'train'
    next_state = rl.get_discrete_state()
    reward = 1.0
    done = False
    
    # è®¡ç®—ä¼˜å…ˆçº§
    priority = rl._calculate_priority(state, action, reward, next_state, done)
    
    # æ·»åŠ åˆ°ç¼“å†²åŒº
    rl.replay_buffer.append((state, action, reward, next_state, done))
    rl.priorities.append(priority)
    
print(f"âœ“ æ·»åŠ äº† {len(rl.replay_buffer)} æ¡ç»éªŒ")
print(f"âœ“ priorities é•¿åº¦: {len(rl.priorities)}")
print(f"âœ“ replay_buffer é•¿åº¦: {len(rl.replay_buffer)}")

# æµ‹è¯•é‡‡æ ·
sample_indices = rl._prioritized_sample(5)
print(f"âœ“ é‡‡æ ·ç»“æœ: {sample_indices}")
print(f"âœ“ é‡‡æ ·æ•°é‡: {len(sample_indices)}")

# æµ‹è¯•åœºæ™¯3ï¼šæ•…æ„åˆ¶é€ é•¿åº¦ä¸ä¸€è‡´çš„æƒ…å†µ
print("\n=== æµ‹è¯•åœºæ™¯3ï¼šæ•…æ„åˆ¶é€ é•¿åº¦ä¸ä¸€è‡´å¹¶æµ‹è¯•ä¿®å¤ ===")

# æ•…æ„è®© priorities æ¯” replay_buffer é•¿
rl.priorities.append(0.1)
rl.priorities.append(0.2)
print(f"âœ— åˆ¶é€ ä¸ä¸€è‡´: priorities={len(rl.priorities)}, replay_buffer={len(rl.replay_buffer)}")

# æµ‹è¯•é‡‡æ ·ï¼ˆåº”è¯¥è‡ªåŠ¨ä¿®å¤ï¼‰
sample_indices = rl._prioritized_sample(5)
print(f"âœ“ é‡‡æ ·å: priorities={len(rl.priorities)}, replay_buffer={len(rl.replay_buffer)}")
print(f"âœ“ é‡‡æ ·ç»“æœ: {sample_indices}")
print(f"âœ“ é‡‡æ ·æ•°é‡: {len(sample_indices)}")

# æµ‹è¯•åœºæ™¯4ï¼šæ•…æ„è®© replay_buffer æ¯” priorities é•¿
print("\n=== æµ‹è¯•åœºæ™¯4ï¼šæ•…æ„è®© replay_buffer æ¯” priorities é•¿ ===")

# æ•…æ„è®© replay_buffer æ¯” priorities é•¿
state = rl.get_discrete_state()
action = 'train'
next_state = rl.get_discrete_state()
reward = 1.0
done = False
rl.replay_buffer.append((state, action, reward, next_state, done))
rl.replay_buffer.append((state, action, reward, next_state, done))
print(f"âœ— åˆ¶é€ ä¸ä¸€è‡´: priorities={len(rl.priorities)}, replay_buffer={len(rl.replay_buffer)}")

# æµ‹è¯•é‡‡æ ·ï¼ˆåº”è¯¥è‡ªåŠ¨ä¿®å¤ï¼‰
sample_indices = rl._prioritized_sample(5)
print(f"âœ“ é‡‡æ ·å: priorities={len(rl.priorities)}, replay_buffer={len(rl.replay_buffer)}")
print(f"âœ“ é‡‡æ ·ç»“æœ: {sample_indices}")
print(f"âœ“ é‡‡æ ·æ•°é‡: {len(sample_indices)}")

print("\n=== æµ‹è¯•å®Œæˆ ===")
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼_prioritized_sample æ–¹æ³•çš„ä¿®å¤æˆåŠŸï¼")
print("âœ… è®­ç»ƒ 'speed' æŠ€èƒ½ä¸å†ä¼šå‡ºç° ValueError é”™è¯¯ã€‚")
