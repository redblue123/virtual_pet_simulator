import time
from collections import defaultdict, Counter
from .base import Pet
from .config import PetConfig
from .systems.decision import DecisionSystem
from .systems.behavior import BehaviorSystem, BehaviorTreeBuilder
from .systems.learning import LearningSystem
from .systems.reinforcement import ReinforcementLearningSystem

class IntelligentPet(Pet):
    """æ™ºèƒ½å® ç‰©ç±» - ç¬¬äºŒé˜¶æ®µå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“"""
    def __init__(self, name="æœªå‘½å", species="æœªçŸ¥"):
        super().__init__(name, species)
        
        # æ™ºèƒ½ä½“ç›¸å…³å±æ€§
        self.decision_system = DecisionSystem(self)
        self.behavior_system = BehaviorSystem(self)
        self.learning_system = LearningSystem(self)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ
        self.reinforcement_learning = ReinforcementLearningSystem(self)
        
        # ç¬¬äºŒé˜¶æ®µï¼šè¡Œä¸ºæ ‘ç³»ç»Ÿ
        self.behavior_tree = BehaviorTreeBuilder.build_pet_behavior_tree()
        
        # ä¸»åŠ¨è¡Œä¸ºç›¸å…³
        self.last_spontaneous_action = time.time()
        self.spontaneous_action_cooldown = PetConfig.SPONTANEOUS_ACTION_COOLDOWN  # è‡ªå‘è¡Œä¸ºå†·å´æ—¶é—´ï¼ˆç§’ï¼‰
        
        # ç”¨æˆ·åå¥½è®°å½•
        self.user_preferences = defaultdict(Counter)
        
        print(f"ğŸ§  æ™ºèƒ½å® ç‰© {name} å·²æ¿€æ´»ï¼")
        print(f"ğŸš€ å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿå·²å¯åŠ¨ï¼")
        print(f"ğŸŒ³ è¡Œä¸ºæ ‘ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼")
    
    def update(self, current_time=None):
        """æ›´æ–°å® ç‰©çŠ¶æ€ï¼ŒåŒ…æ‹¬æ™ºèƒ½ä½“å†³ç­–"""
        super().update(current_time)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œè‡ªå‘è¡Œä¸º
        if current_time is None:
            current_time = time.time()
        
        # é˜²æ­¢é€’å½’è°ƒç”¨ï¼šåªåœ¨éé€’å½’è°ƒç”¨æ—¶æ‰§è¡Œè‡ªå‘è¡Œä¸º
        if not hasattr(self, '_updating') or not self._updating:
            if current_time - self.last_spontaneous_action > self.spontaneous_action_cooldown:
                self._updating = True
                try:
                    self.execute_spontaneous_action()
                    self.last_spontaneous_action = current_time
                finally:
                    self._updating = False
    
    def execute_spontaneous_action(self):
        """æ‰§è¡Œè‡ªå‘è¡Œä¸ºï¼ˆä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œè¡Œä¸ºæ ‘ï¼‰"""
        # ç¬¬äºŒé˜¶æ®µï¼šä¼˜å…ˆä½¿ç”¨å¼ºåŒ–å­¦ä¹ å†³ç­–
        # 1. è·å–å½“å‰çš„ç¦»æ•£çŠ¶æ€ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ å†³ç­–
        state_before = self.reinforcement_learning.get_discrete_state()
        # 2. ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
        action = self.reinforcement_learning.choose_action(state_before)
        
        if action:
            # 3. æ‰§è¡Œé€‰æ‹©çš„è¡Œä¸º
            result = self._execute_action(action)
            
            # 4. è¯„ä¼°æ‰§è¡Œåçš„çŠ¶æ€
            state_after = self.reinforcement_learning.get_discrete_state()
            
            # 5. è®¡ç®—å¥–åŠ± - ç›´æ¥ä½¿ç”¨åŸå§‹çŠ¶æ€å­—å…¸
            # è·å–æ‰§è¡Œå‰åçš„è¯¦ç»†çŠ¶æ€ï¼Œä½¿ç”¨ force_update=False é¿å…ä¸å¿…è¦çš„æ›´æ–°
            status_before = self.get_status(force_update=False)
            status_after = self.get_status(force_update=False)
            # æå–éœ€è¦çš„çŠ¶æ€å€¼ï¼ˆå¦‚é¥¥é¥¿ã€èƒ½é‡ã€æ¸…æ´åº¦ç­‰ï¼‰
            state_dict_before = self._extract_state_values(status_before)
            state_dict_after = self._extract_state_values(status_after)
            # ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿè®¡ç®—å¥–åŠ±
            reward = self.reinforcement_learning.calculate_reward(
                state_dict_before, action, state_dict_after
            )
            
            # 6. å¼ºåŒ–å­¦ä¹  - æ ¹æ®æ‰§è¡Œç»“æœæ›´æ–°Q-table
            self.reinforcement_learning.learn(state_before, action, reward, state_after, False)
            
            # 7. è®°å½•è¡Œä¸ºç»“æœåˆ°å­¦ä¹ ç³»ç»Ÿ
            self.learning_system.record_behavior(action, result, {})
            
            return result
        else:
            # å¤‡ç”¨ï¼šå¦‚æœå¼ºåŒ–å­¦ä¹ æ²¡æœ‰é€‰æ‹©åŠ¨ä½œï¼Œä½¿ç”¨è¡Œä¸ºæ ‘
            return self.execute_behavior_tree_action()
    
    def execute_behavior_tree_action(self):
        """æ‰§è¡Œè¡Œä¸ºæ ‘åŠ¨ä½œ"""
        status = self.behavior_tree.execute(self)
        return f"è¡Œä¸ºæ ‘æ‰§è¡ŒçŠ¶æ€: {status}"
    
    def _execute_action(self, action):
        """æ‰§è¡Œå…·ä½“è¡Œä¸º"""
        if action == "feed":
            return self.feed("æ™®é€šé£Ÿç‰©")
        elif action == "play":
            return self.play("æ™®é€šæ¸¸æˆ")
        elif action == "sleep":
            return self.sleep()
        elif action == "clean":
            return self.clean()
        elif action == "train":
            return self.train("intelligence")
        elif action == "explore":
            return self._explore()
        elif action == "rest":
            return self._rest()
        else:
            return f"æ‰§è¡Œè¡Œä¸º: {action}"
    
    def interact_with_user(self, interaction_type, **kwargs):
        """ä¸ç”¨æˆ·äº¤äº’å¹¶å­¦ä¹ """
        # æ‰§è¡Œä¼ ç»Ÿäº¤äº’
        if interaction_type == "feed":
            food_type = kwargs.get("food_type", "æ™®é€šé£Ÿç‰©")
            result = self.feed(food_type)
        elif interaction_type == "play":
            game_type = kwargs.get("game_type", "æ™®é€šæ¸¸æˆ")
            result = self.play(game_type)
        elif interaction_type == "sleep":
            result = self.sleep()
        elif interaction_type == "wake_up":
            result = self.wake_up()
        elif interaction_type == "clean":
            clean_type = kwargs.get("clean_type", "æ¯›å‘æ¸…ç†")
            result = self.clean(clean_type)
        elif interaction_type == "train":
            skill_type = kwargs.get("skill_type", "intelligence")
            result = self.train(skill_type)
        elif interaction_type == "change_color":
            new_color = kwargs.get("new_color", "ç™½è‰²")
            result = self.change_color(new_color)
        else:
            result = "æœªçŸ¥äº¤äº’ç±»å‹"
        
        # è®°å½•ç”¨æˆ·äº¤äº’åå¥½
        self.learning_system.record_user_interaction(interaction_type, kwargs)
        
        # å­¦ä¹ ç³»ç»Ÿæ›´æ–°
        self.learning_system.learn_from_interaction(interaction_type, result)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¼ºåŒ–å­¦ä¹ æ›´æ–°
        state_before = self.reinforcement_learning.get_discrete_state()
        # å°†ç”¨æˆ·äº¤äº’æ˜ å°„åˆ°å¼ºåŒ–å­¦ä¹ åŠ¨ä½œ
        rl_action = self._map_interaction_to_rl_action(interaction_type)
        if rl_action:
            state_after = self.reinforcement_learning.get_discrete_state()
            # ç›´æ¥ä½¿ç”¨åŸå§‹çŠ¶æ€å­—å…¸è®¡ç®—å¥–åŠ±
            status_before = self.get_status()
            status_after = self.get_status()
            # æå–éœ€è¦çš„çŠ¶æ€å€¼
            state_dict_before = self._extract_state_values(status_before)
            state_dict_after = self._extract_state_values(status_after)
            reward = self.reinforcement_learning.calculate_reward(
                state_dict_before, rl_action, state_dict_after
            )
            self.reinforcement_learning.learn(state_before, rl_action, reward, state_after, False)
        
        return result
    
    def _extract_state_values(self, status):
        """ä»çŠ¶æ€å­—å…¸ä¸­æå–éœ€è¦çš„æ•°å€¼"""
        return {
            "hunger": float(status["hunger"].split("/")[0]),
            "energy": float(status["energy"].split("/")[0]),
            "hygiene": float(status["hygiene"].split("/")[0]),
            "happiness": float(status["happiness"].split("/")[0]),
            "health": float(status["health"].split("/")[0])
        }
    
    def _map_interaction_to_rl_action(self, interaction_type):
        """å°†ç”¨æˆ·äº¤äº’æ˜ å°„åˆ°å¼ºåŒ–å­¦ä¹ åŠ¨ä½œ"""
        mapping = {
            "feed": "feed",
            "play": "play",
            "sleep": "sleep",
            "wake_up": "explore",  # é†’æ¥åæ¢ç´¢
            "clean": "clean",
            "train": "train"
        }
        return mapping.get(interaction_type)
    
    def get_intelligent_status(self):
        """è·å–æ™ºèƒ½ä½“çŠ¶æ€"""
        base_status = self.get_status()
        
        # æ·»åŠ æ™ºèƒ½ä½“ç›¸å…³ä¿¡æ¯
        intelligent_status = {
            "decision_confidence": self.decision_system.get_confidence(),
            "predicted_needs": self.decision_system.predict_needs(),
            "learned_preferences": dict(self.learning_system.get_preferences()),
            "spontaneous_action_rate": self.behavior_system.get_action_rate(),
            "next_action_prediction": self.decision_system.predict_next_action(),
            # ç¬¬äºŒé˜¶æ®µï¼šå¼ºåŒ–å­¦ä¹ ä¿¡æ¯
            "reinforcement_learning": self.reinforcement_learning.get_learning_stats()
        }
        
        base_status.update(intelligent_status)
        return base_status
    
    def get_learning_progress(self):
        """è·å–å­¦ä¹ è¿›åº¦"""
        return {
            "exploration_rate": self.reinforcement_learning.exploration_rate,
            "average_reward": self.reinforcement_learning.average_reward,
            "learning_steps": self.reinforcement_learning.learning_steps,
            "q_table_size": sum(len(v) for v in self.reinforcement_learning.q_table.values())
        }
    
    def beg_for_food(self):
        """å‘ä¸»äººä¹è®¨é£Ÿç‰©"""
        self.happiness += 5  # ä¹è®¨è¡Œä¸ºå¢åŠ ä¸€ç‚¹å¿«ä¹
        return f"{self.name}ï¼š'æˆ‘é¥¿äº†ï¼Œæƒ³åƒä¸œè¥¿ï¼'"
    
    def groom(self):
        """è‡ªæˆ‘æ¸…æ´"""
        hygiene_gain = 15
        self.hygiene = min(100, self.hygiene + hygiene_gain)
        self.happiness += 5
        return f"{self.name}æ­£åœ¨èˆ”æ¯›æ¸…æ´è‡ªå·±"
    
    def spontaneous_play(self):
        """è‡ªå‘ç©è€"""
        if self.energy < 20:
            return f"{self.name}ï¼š'æˆ‘å¤ªç´¯äº†ï¼Œæƒ³ä¼‘æ¯'"
        
        energy_cost = 10
        happiness_gain = 15
        
        self.energy = max(0, self.energy - energy_cost)
        self.happiness = min(100, self.happiness + happiness_gain)
        
        return f"{self.name}æ­£åœ¨å¼€å¿ƒåœ°ç©è€"
